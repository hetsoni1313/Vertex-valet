# Vertex-Valet 

A Big Data Engineering project for processing, storing, and serving book data. This project demonstrates an end-to-end ETL (Extract, Transform, Load) pipeline for book recommendations or catalog management, culminating in a REST API and a Semantic Search Engine.

## Features

- **Data Ingestion**: Multi-source ingestion pipeline to load raw book data.
- **Data Transformation**: Clean and process data, handle missing strings, and normalize formats.
- **Data Storage**: Efficient storage using SQLite with optimized schema.
- **API Service**: Fast, asynchronous REST API built with **FastAPI**.
- **Recommender System**: Semantic search capabilities using **Sentence Transformers** (`all-MiniLM-L6-v2`) to find books based on natural language descriptions (e.g., "sad story about a robot").
- **Hybrid Search**: "Smart Search" that combines keyword matching (for Authors) with semantic similarity.
- **Frontend**: A modern, responsive web interface for users to explore books.

## ğŸ“ Project Structure

```
Vertex-valet/
â”œâ”€â”€ pipeline.py              # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ frontend/                # Web Interface
â”‚   â”œâ”€â”€ index.html           # Main UI
â”‚   â”œâ”€â”€ app.js               # Frontend Logic
â”‚   â””â”€â”€ styles.css           # Styling
â”‚
â”œâ”€â”€ recommender/             # Recommendation Engine
â”‚   â”œâ”€â”€ build_embeddings.py  # Script to generate embeddings
â”‚   â”œâ”€â”€ recommender.py       # Inference engine
â”‚   â”œâ”€â”€ patch_metadata.py    # Utility to update metadata
â”‚   â”œâ”€â”€ embeddings.pkl       # Vector artifacts (generated)
â”‚   â””â”€â”€ README.md            # Specific documentation
â”‚
â”œâ”€â”€ API/
â”‚   â””â”€â”€ main.py              # FastAPI application
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingestion.py         # Data ingestion module
â”‚
â”œâ”€â”€ transformation/
â”‚   â””â”€â”€ transformation.py    # Data transformation module
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ db.py                # Database operations module
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw input (RC_books.csv)
â”‚   â””â”€â”€ processed/           # Cleaned CSVs
â”‚
â””â”€â”€ logs/                    # System logs
```

## Installation

### Prerequisites
- Python 3.8 or higher

### Setup

1. **Create Virtual Environment**:
   ```bash
   python -m venv myvenv
   .\myvenv\Scripts\activate  # Windows
   # source myvenv/bin/activate  # Linux/Mac
   ```

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Initialize Database** (First time only):
   ```bash
   python pipeline.py --db
   ```

4. **Generate Embeddings** (Required for Recommender):
   ```bash
   python recommender/build_embeddings.py
   ```
   *Note: This downloads the 90MB model and may take a few minutes to process ~28k books.*

---

## ğŸ§  Recommender System

This module provides the intelligence behind the "Recommend" feature.

- **Model**: Uses `all-MiniLM-L6-v2` (a lightweight, high-performance transformer).
- **Process**: 
  1. Concatenates Book Title + Description.
  2. Generates dense vector embeddings.
  3. Calculates Cosine Similarity between user query and book vectors.
- **Hybrid Logic**: The engine prioritizes **Exact Author Matches** (boosting their score) while mixing in semantic results, allowing users to search by both "Vibe" and "Author Name" in a single bar.

You can test the recommender standalone:
```bash
python recommender/recommender.py
```

---

## ğŸ’» Usage

### 1. Run the Full Backend Pipeline
To run the API (which automatically loads the recommender):
```bash
python pipeline.py --api
```
The API listens at `http://127.0.0.1:8000`.

### 2. Run the Frontend
In a separate terminal:
```bash
cd frontend
python -m http.server 3000
```
Open **[http://localhost:3000](http://localhost:3000)** in your browser.

### 3. Pipeline Commands
The `pipeline.py` script helps manage the ETL process:

- **Run Ingestion**: `python pipeline.py --ingestion`
- **Run Transformation**: `python pipeline.py --transformation`
- **Reset Database**: `python pipeline.py --db`
- **Run Everything**: `python pipeline.py --all`

---

## ğŸ“Š Data Snapshot

- **Raw Data**: ~36,361 records
- **Cleaned & Indexed**: ~28,503 records (Filtered for valid ISBNs and Descriptions)
- **Sources**: OpenLibrary, Google Books, Bookswagon (Data resources).

---

## ğŸ” API Endpoints

- **GET /**: Health check (`{"status": "API is running"}`)
- **GET /recommend?query=...**: Semantic/Hybrid search.
- **GET /books/{isbn}**: Get details by ISBN.
- **GET /search?q=...**: Legacy keyword search (Title/Author).

---

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes.

<div align="right">

**Vertex-Valet Team**
**Het Katrodiya**
**Gaurang Jadav**

</div>
